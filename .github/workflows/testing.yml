name: Automated Testing

on:
  push:
    branches: [ main, develop, feature/* ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run tests daily at 6 AM UTC
    - cron: '0 6 * * *'

env:
  PYTHON_VERSION: 3.9

jobs:
  # ================================
  # UNIT TESTS
  # ================================
  unit-tests:
    name: Unit Tests
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: [3.8, 3.9, '3.10']
        exclude:
          # Reduce matrix size for faster execution
          - os: windows-latest
            python-version: 3.8
          - os: macos-latest
            python-version: 3.8
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-xdist pytest-mock

    - name: Create test structure
      run: |
        mkdir -p tests
        
        # Create basic unit tests
        cat > tests/test_model.py << 'EOF'
        import pytest
        import torch
        import numpy as np
        from model import TabularMLP

        def test_model_creation():
            model = TabularMLP(input_dim=10)
            assert model is not None
            assert isinstance(model, torch.nn.Module)

        def test_model_forward():
            model = TabularMLP(input_dim=10)
            x = torch.randn(5, 10)
            output = model(x)
            assert output.shape == (5,)
            assert torch.all(output >= 0) and torch.all(output <= 1)

        def test_model_different_sizes():
            for input_dim in [5, 20, 50, 100]:
                model = TabularMLP(input_dim=input_dim)
                x = torch.randn(3, input_dim)
                output = model(x)
                assert output.shape == (3,)
        EOF
        
        cat > tests/test_drift_detector.py << 'EOF'
        import pytest
        import pandas as pd
        import numpy as np
        from drift_detector import detect_drift_featurewise, psi

        def test_psi_calculation():
            baseline = np.random.normal(0, 1, 1000)
            current = np.random.normal(0, 1, 1000)
            psi_val = psi(baseline, current)
            assert isinstance(psi_val, float)
            assert psi_val >= 0

        def test_drift_detection():
            baseline_df = pd.DataFrame({
                'feature1': np.random.normal(0, 1, 100),
                'feature2': np.random.normal(0, 1, 100)
            })
            current_df = pd.DataFrame({
                'feature1': np.random.normal(0.5, 1, 100),  # Shifted
                'feature2': np.random.normal(0, 1, 100)    # Same
            })
            
            results = detect_drift_featurewise(baseline_df, current_df)
            assert 'feature1' in results
            assert 'feature2' in results
            assert 'psi' in results['feature1']
            assert 'ks_pvalue' in results['feature1']
            assert 'drift_flag' in results['feature1']

        def test_drift_no_drift():
            baseline_df = pd.DataFrame({
                'feature1': np.random.normal(0, 1, 100)
            })
            current_df = pd.DataFrame({
                'feature1': np.random.normal(0, 1, 100)
            })
            
            results = detect_drift_featurewise(baseline_df, current_df)
            # With same distribution, drift should be minimal
            assert results['feature1']['psi'] < 1.0
        EOF
        
        cat > tests/test_data_ingestion.py << 'EOF'
        import pytest
        from unittest.mock import patch, MagicMock
        import pandas as pd
        import numpy as np

        def test_load_federated_data_mock():
            # Mock the flwr_datasets to avoid actual download
            with patch('data_ingestion.FederatedDataset') as mock_fds:
                mock_partition = MagicMock()
                mock_partition.to_pandas.return_value = pd.DataFrame({
                    'age': np.random.randint(18, 80, 100),
                    'income': np.random.choice([0, 1], 100)
                })
                
                mock_fds.return_value.load_partition.return_value = mock_partition
                
                from data_ingestion import load_federated_data
                data = load_federated_data(num_clients=2)
                
                assert len(data) == 2
                for cid in range(2):
                    assert cid in data
                    assert 'X_train_norm' in data[cid]
                    assert 'y_train' in data[cid]
        EOF
        
        cat > tests/conftest.py << 'EOF'
        import pytest
        import tempfile
        import os
        import shutil

        @pytest.fixture
        def temp_dir():
            temp_dir = tempfile.mkdtemp()
            yield temp_dir
            shutil.rmtree(temp_dir)

        @pytest.fixture
        def sample_model_data():
            import torch
            from model import TabularMLP
            
            model = TabularMLP(input_dim=10)
            X = torch.randn(50, 10)
            y = torch.randint(0, 2, (50,)).float()
            
            return model, X, y
        EOF

    - name: Run unit tests
      run: |
        pytest tests/ -v --cov=. --cov-report=xml --cov-report=term-missing --tb=short

    - name: Upload coverage to artifact
      if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.9'
      uses: actions/upload-artifact@v3
      with:
        name: coverage-report
        path: coverage.xml

  # ================================
  # INTEGRATION TESTS
  # ================================
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    
    services:
      # Add services needed for integration tests
      redis:
        image: redis:alpine
        ports:
          - 6379:6379
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest requests

    - name: Create integration tests
      run: |
        mkdir -p tests/integration
        
        cat > tests/integration/test_api_integration.py << 'EOF'
        import pytest
        import requests
        import json
        import time
        import subprocess
        import threading
        from unittest.mock import patch

        class TestAPIIntegration:
            
            @pytest.fixture(autouse=True)
            def setup_mock_data(self):
                # Create mock client datasets and models
                import pickle
                import torch
                from model import TabularMLP
                import numpy as np
                import pandas as pd
                
                # Create mock client datasets
                mock_data = {
                    0: {
                        'X_train_raw': pd.DataFrame(np.random.randn(100, 10)),
                        'X_test_raw': pd.DataFrame(np.random.randn(20, 10)),
                        'X_train_norm': np.random.randn(100, 10).astype(np.float32),
                        'X_test_norm': np.random.randn(20, 10).astype(np.float32),
                        'y_train': np.random.randint(0, 2, 100).astype(np.float32),
                        'y_test': np.random.randint(0, 2, 20).astype(np.float32),
                    }
                }
                
                with open('client_datasets.pkl', 'wb') as f:
                    pickle.dump(mock_data, f)
                
                # Create mock model
                import os
                os.makedirs('models', exist_ok=True)
                model = TabularMLP(input_dim=10)
                torch.save(model.state_dict(), 'models/test_model.pt')
                
                yield
                
                # Cleanup
                try:
                    os.remove('client_datasets.pkl')
                    os.remove('models/test_model.pt')
                    os.rmdir('models')
                except:
                    pass

            def test_end_to_end_training(self):
                # Test the complete training pipeline
                from data_ingestion import load_federated_data
                from save_clients import main as save_clients_main
                
                # Mock federated data loading
                with patch('data_ingestion.FederatedDataset'):
                    # This would test the full pipeline
                    assert True  # Placeholder

            def test_drift_detection_pipeline(self):
                # Test drift detection integration
                import pandas as pd
                import numpy as np
                from drift_detector import detect_drift_featurewise
                
                baseline = pd.DataFrame(np.random.randn(100, 5))
                current = pd.DataFrame(np.random.randn(100, 5))
                
                results = detect_drift_featurewise(baseline, current)
                assert len(results) == 5
                
        EOF
        
        cat > tests/integration/test_federated_learning.py << 'EOF'
        import pytest
        import multiprocessing
        import time
        from unittest.mock import patch, MagicMock

        def test_fl_client_server_integration():
            # Mock FL integration test
            from fl_client import FLClient
            from model import TabularMLP
            import numpy as np
            
            # Create mock client data
            mock_data = {
                'X_train_norm': np.random.randn(50, 10).astype(np.float32),
                'X_test_norm': np.random.randn(10, 10).astype(np.float32),
                'y_train': np.random.randint(0, 2, 50).astype(np.float32),
                'y_test': np.random.randint(0, 2, 10).astype(np.float32),
            }
            
            # Test client creation
            client = FLClient(cid=0, client_data=mock_data, input_dim=10)
            assert client.cid == 0
            assert client.model is not None
            
            # Test parameter extraction
            params = client.get_parameters()
            assert len(params) > 0
            
            # Test parameter setting
            client.set_parameters(params)
            
        EOF

    - name: Run integration tests
      run: |
        pytest tests/integration/ -v --tb=short

  # ================================
  # END-TO-END TESTS
  # ================================
  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    needs: integration-tests
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Build test images
      run: |
        echo "ðŸ³ Building Docker images for E2E testing..."
        docker-compose -f docker-compose.yml build

    - name: Run E2E test scenario
      run: |
        echo "ðŸš€ Running E2E test scenario..."
        
        # Create test script
        cat > e2e_test.py << 'EOF'
        import time
        import requests
        import subprocess
        import os
        import signal

        def test_full_system():
            print("ðŸ§ª Starting full system E2E test...")
            
            # Start system
            print("Starting Docker services...")
            proc = subprocess.Popen(
                ["docker-compose", "up", "-d"],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            )
            proc.wait()
            
            # Wait for services to start
            print("Waiting for services to start...")
            time.sleep(30)
            
            try:
                # Test API health
                print("Testing API health...")
                response = requests.get("http://localhost:8000/health", timeout=10)
                assert response.status_code == 200, f"API health check failed: {response.status_code}"
                print("âœ… API health check passed")
                
                # Test dashboard accessibility
                print("Testing dashboard accessibility...")
                response = requests.get("http://localhost:8501", timeout=10)
                # Streamlit may return different status codes, so just check if it responds
                print(f"Dashboard responded with status: {response.status_code}")
                
                # Test Prometheus metrics
                print("Testing Prometheus metrics...")
                response = requests.get("http://localhost:9090/-/healthy", timeout=10)
                print(f"Prometheus health: {response.status_code}")
                
                print("ðŸŽ‰ E2E test completed successfully!")
                
            except Exception as e:
                print(f"âŒ E2E test failed: {e}")
                raise
            finally:
                # Clean up
                print("Cleaning up Docker services...")
                subprocess.run(["docker-compose", "down", "-v"], check=False)

        if __name__ == "__main__":
            test_full_system()
        EOF
        
        # Run the E2E test
        timeout 300 python e2e_test.py || {
            echo "âŒ E2E test failed or timed out"
            docker-compose logs
            docker-compose down -v
            exit 1
        }

  # ================================
  # PERFORMANCE TESTS
  # ================================
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-benchmark memory-profiler

    - name: Create performance tests
      run: |
        mkdir -p tests/performance
        
        cat > tests/performance/test_model_performance.py << 'EOF'
        import pytest
        import torch
        import numpy as np
        from model import TabularMLP
        from train_utils import train_local, evaluate_model

        class TestModelPerformance:
            
            def test_model_inference_speed(self, benchmark):
                model = TabularMLP(input_dim=50)
                model.eval()
                x = torch.randn(100, 50)
                
                def inference():
                    with torch.no_grad():
                        return model(x)
                
                result = benchmark(inference)
                # Check that we can process 100 samples reasonably quickly
                assert result.shape == (100,)

            def test_training_speed(self, benchmark):
                model = TabularMLP(input_dim=20)
                X = np.random.randn(500, 20).astype(np.float32)
                y = np.random.randint(0, 2, 500).astype(np.float32)
                
                def train():
                    train_local(model, X, y, epochs=1)
                    return model
                
                result = benchmark(train)
                assert result is not None

            @pytest.mark.parametrize("batch_size", [32, 64, 128, 256])
            def test_batch_size_performance(self, batch_size, benchmark):
                model = TabularMLP(input_dim=30)
                x = torch.randn(batch_size, 30)
                
                def process_batch():
                    with torch.no_grad():
                        return model(x)
                
                result = benchmark(process_batch)
                assert result.shape == (batch_size,)

        EOF

    - name: Run performance tests
      run: |
        pytest tests/performance/ -v --benchmark-only --benchmark-sort=mean

  # ================================
  # SECURITY TESTS
  # ================================
  security-tests:
    name: Security Tests  
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install security testing tools
      run: |
        python -m pip install --upgrade pip
        pip install safety bandit semgrep

    - name: Run dependency security scan
      run: |
        echo "ðŸ”’ Running dependency security scan..."
        safety check --json --output safety-report.json || true
        
        if [ -f safety-report.json ]; then
          echo "Safety scan completed. Results:"
          cat safety-report.json
        fi

    - name: Run code security analysis
      run: |
        echo "ðŸ” Running code security analysis..."
        bandit -r . -f json -o bandit-report.json || true
        
        if [ -f bandit-report.json ]; then
          echo "Bandit scan completed. Results:"
          cat bandit-report.json
        fi

    - name: Check for secrets in code
      run: |
        echo "ðŸ” Checking for hardcoded secrets..."
        
        # Basic secret detection patterns
        if grep -r -i -E "(password|token|key|secret|api_key).*=.*['\"][^'\"]{8,}" . \
           --exclude-dir=.git \
           --exclude-dir=.github \
           --exclude="*.json" \
           --exclude="*.yml" \
           --exclude="*.yaml" || true; then
          echo "âš ï¸ Potential secrets found in code"
        else
          echo "âœ… No hardcoded secrets detected"
        fi

    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports
        path: |
          safety-report.json
          bandit-report.json

  # ================================
  # TEST REPORT AGGREGATION
  # ================================
  test-report:
    name: Test Report Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, e2e-tests, performance-tests, security-tests]
    if: always()
    
    steps:
    - name: Download test artifacts
      uses: actions/download-artifact@v3
      continue-on-error: true

    - name: Generate test summary
      run: |
        echo "ðŸ“Š Test Execution Summary" > test-summary.md
        echo "=========================" >> test-summary.md
        echo "" >> test-summary.md
        echo "**Test Run:** ${{ github.run_id }}" >> test-summary.md
        echo "**Commit:** ${{ github.sha }}" >> test-summary.md
        echo "**Branch:** ${{ github.ref_name }}" >> test-summary.md
        echo "**Trigger:** ${{ github.event_name }}" >> test-summary.md
        echo "" >> test-summary.md
        
        echo "## Test Results" >> test-summary.md
        echo "" >> test-summary.md
        
        # Check job results
        unit_result="${{ needs.unit-tests.result }}"
        integration_result="${{ needs.integration-tests.result }}"
        e2e_result="${{ needs.e2e-tests.result }}"
        performance_result="${{ needs.performance-tests.result }}"
        security_result="${{ needs.security-tests.result }}"
        
        echo "| Test Type | Status | " >> test-summary.md
        echo "|-----------|--------|" >> test-summary.md
        echo "| Unit Tests | $unit_result |" >> test-summary.md
        echo "| Integration Tests | $integration_result |" >> test-summary.md
        echo "| End-to-End Tests | $e2e_result |" >> test-summary.md
        echo "| Performance Tests | $performance_result |" >> test-summary.md
        echo "| Security Tests | $security_result |" >> test-summary.md
        echo "" >> test-summary.md
        
        # Overall status
        if [[ "$unit_result" == "success" && "$integration_result" == "success" && "$e2e_result" == "success" ]]; then
          echo "## âœ… Overall Status: PASSED" >> test-summary.md
          echo "All critical tests passed successfully!" >> test-summary.md
        else
          echo "## âŒ Overall Status: FAILED" >> test-summary.md
          echo "Some tests failed. Please review the results above." >> test-summary.md
        fi
        
        echo "" >> test-summary.md
        echo "---" >> test-summary.md
        echo "*Generated on $(date)*" >> test-summary.md
        
        cat test-summary.md

    - name: Upload test summary
      uses: actions/upload-artifact@v3
      with:
        name: test-summary
        path: test-summary.md

    - name: Comment on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const summary = fs.readFileSync('test-summary.md', 'utf8');
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: summary
          });

    - name: Update status check
      if: always()
      run: |
        # In a real scenario, you would update commit status
        echo "Test execution completed"
        echo "Unit Tests: ${{ needs.unit-tests.result }}"
        echo "Integration Tests: ${{ needs.integration-tests.result }}"
        echo "E2E Tests: ${{ needs.e2e-tests.result }}"
        echo "Performance Tests: ${{ needs.performance-tests.result }}"
        echo "Security Tests: ${{ needs.security-tests.result }}"